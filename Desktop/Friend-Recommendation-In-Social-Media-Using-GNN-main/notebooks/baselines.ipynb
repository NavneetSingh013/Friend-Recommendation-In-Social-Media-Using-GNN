{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Methods\n",
        "\n",
        "This notebook demonstrates baseline methods for friend recommendation:\n",
        "1. Common Neighbors\n",
        "2. Jaccard Coefficient\n",
        "3. Adamic-Adar\n",
        "4. Preferential Attachment\n",
        "5. Node2Vec\n",
        "6. Matrix Factorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from src.data.heuristics import compute_heuristics, predict_links_heuristics\n",
        "from src.baselines.node2vec import Node2VecBaseline\n",
        "from src.baselines.matrix_factorization import MatrixFactorizationBaseline\n",
        "from src.evaluation import compute_metrics, compute_ranking_metrics\n",
        "\n",
        "# Load data\n",
        "data = torch.load(\"data/processed/facebook_combined.pt\")\n",
        "link_data = torch.load(\"data/processed/facebook_link_data.pt\")\n",
        "\n",
        "print(f\"Graph: {data.num_nodes} nodes, {data.edge_index.size(1) // 2} edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load or compute heuristics\n",
        "try:\n",
        "    heuristics = torch.load(\"data/processed/facebook_heuristics.pt\")\n",
        "    print(\"Loaded precomputed heuristics\")\n",
        "except:\n",
        "    print(\"Computing heuristics...\")\n",
        "    heuristics = compute_heuristics(data)\n",
        "    torch.save(heuristics, \"data/processed/facebook_heuristics.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate heuristics on test set\n",
        "test_edges = link_data['test_edges']\n",
        "test_labels = link_data['test_labels'].numpy()\n",
        "\n",
        "results = {}\n",
        "for method_name, scores_dict in heuristics.items():\n",
        "    # Get scores for test edges\n",
        "    test_scores = []\n",
        "    for i in range(test_edges.size(1)):\n",
        "        src, dst = test_edges[0, i].item(), test_edges[1, i].item()\n",
        "        edge = (src, dst)\n",
        "        if edge in scores_dict:\n",
        "            test_scores.append(scores_dict[edge])\n",
        "        else:\n",
        "            test_scores.append(0.0)\n",
        "    \n",
        "    test_scores = np.array(test_scores)\n",
        "    # Normalize scores\n",
        "    if test_scores.max() > 0:\n",
        "        test_scores = test_scores / test_scores.max()\n",
        "    \n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(test_scores, test_labels)\n",
        "    ranking_metrics = compute_ranking_metrics(test_scores, test_labels, k_values=[5, 10, 20])\n",
        "    \n",
        "    results[method_name] = {**metrics, **ranking_metrics}\n",
        "    print(f\"{method_name}: AUC={metrics['auc']:.4f}, AP={metrics['ap']:.4f}, P@10={ranking_metrics['precision@10']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Node2Vec baseline\n",
        "print(\"Training Node2Vec...\")\n",
        "node2vec = Node2VecBaseline(dimensions=64, walk_length=30, num_walks=200)\n",
        "node2vec.train(data.edge_index, data.num_nodes)\n",
        "\n",
        "# Evaluate Node2Vec\n",
        "node2vec_scores = node2vec.predict(test_edges)\n",
        "node2vec_scores = (node2vec_scores - node2vec_scores.min()) / (node2vec_scores.max() - node2vec_scores.min() + 1e-8)\n",
        "\n",
        "metrics_node2vec = compute_metrics(node2vec_scores, test_labels)\n",
        "ranking_metrics_node2vec = compute_ranking_metrics(node2vec_scores, test_labels, k_values=[5, 10, 20])\n",
        "\n",
        "results['node2vec'] = {**metrics_node2vec, **ranking_metrics_node2vec}\n",
        "print(f\"Node2Vec: AUC={metrics_node2vec['auc']:.4f}, AP={metrics_node2vec['ap']:.4f}, P@10={ranking_metrics_node2vec['precision@10']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
