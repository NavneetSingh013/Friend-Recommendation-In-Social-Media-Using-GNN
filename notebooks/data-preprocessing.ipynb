{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing Notebook\n",
        "\n",
        "This notebook demonstrates how to download and preprocess datasets for friend recommendation using GNN.\n",
        "\n",
        "## Datasets:\n",
        "1. **SNAP Facebook Social Circles**: Small-scale ego-network dataset\n",
        "2. **OGB Link Prediction**: Large-scale collaboration network\n",
        "3. **Synthetic Dataset**: For quick testing\n",
        "\n",
        "## Steps:\n",
        "1. Download datasets\n",
        "2. Preprocess graphs (node features, edge information)\n",
        "3. Compute heuristics (common neighbors, Jaccard, etc.)\n",
        "4. Prepare link prediction splits (train/val/test)\n",
        "5. Save processed data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from src.data.facebook_loader import FacebookDatasetLoader, create_synthetic_dataset\n",
        "from src.data.ogb_loader import OGBDatasetLoader\n",
        "from src.data.preprocessing import prepare_link_prediction_data\n",
        "from src.data.heuristics import compute_heuristics\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. SNAP Facebook Dataset\n",
        "\n",
        "Let's download and process the Facebook dataset. This is a good starter dataset with ego-networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Facebook loader\n",
        "facebook_loader = FacebookDatasetLoader(data_dir=\"data/raw/facebook\")\n",
        "\n",
        "# Download dataset (this may take a few minutes)\n",
        "# Uncomment to download:\n",
        "# facebook_loader.download()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Facebook dataset\n",
        "# Combine multiple ego networks (using first 10 for faster processing)\n",
        "print(\"Processing Facebook dataset...\")\n",
        "data = facebook_loader.process_and_save(max_egos=10)\n",
        "\n",
        "print(f\"Processed graph: {data.num_nodes} nodes, {data.edge_index.size(1) // 2} edges\")\n",
        "print(f\"Node features: {data.x.shape}\")\n",
        "print(f\"Feature dimension: {data.x.size(1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare link prediction data\n",
        "# This splits edges into train/val/test and generates negative samples\n",
        "print(\"Preparing link prediction data...\")\n",
        "link_data = prepare_link_prediction_data(data, train_ratio=0.7, val_ratio=0.15, neg_ratio=1.0, seed=42)\n",
        "\n",
        "print(f\"Train edges: {link_data['train_edges'].size(1)}\")\n",
        "print(f\"Val edges: {link_data['val_edges'].size(1)}\")\n",
        "print(f\"Test edges: {link_data['test_edges'].size(1)}\")\n",
        "print(f\"Train positive: {link_data['train_pos'].size(1)}, negative: {link_data['train_neg'].size(1)}\")\n",
        "print(f\"Val positive: {link_data['val_pos'].size(1)}, negative: {link_data['val_neg'].size(1)}\")\n",
        "print(f\"Test positive: {link_data['test_pos'].size(1)}, negative: {link_data['test_neg'].size(1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save link prediction data\n",
        "torch.save(link_data, \"data/processed/facebook_link_data.pt\")\n",
        "print(\"Saved link prediction data to data/processed/facebook_link_data.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compute Heuristics\n",
        "\n",
        "Compute baseline heuristics for comparison with GNN models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute heuristics\n",
        "# Note: This can be slow for large graphs\n",
        "print(\"Computing heuristics...\")\n",
        "heuristics = compute_heuristics(data, methods=['common_neighbors', 'jaccard', 'adamic_adar', 'preferential_attachment'])\n",
        "\n",
        "# Save heuristics\n",
        "torch.save(heuristics, \"data/processed/facebook_heuristics.pt\")\n",
        "print(\"Saved heuristics to data/processed/facebook_heuristics.pt\")\n",
        "print(f\"Computed {len(heuristics)} heuristics: {list(heuristics.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Synthetic Dataset (for quick testing)\n",
        "\n",
        "Create a small synthetic dataset for quick experimentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic dataset\n",
        "print(\"Creating synthetic dataset...\")\n",
        "synthetic_data = create_synthetic_dataset(num_nodes=100, num_edges=200, feature_dim=16)\n",
        "\n",
        "print(f\"Synthetic graph: {synthetic_data.num_nodes} nodes, {synthetic_data.edge_index.size(1) // 2} edges\")\n",
        "print(f\"Node features: {synthetic_data.x.shape}\")\n",
        "\n",
        "# Save synthetic data\n",
        "torch.save(synthetic_data, \"data/processed/synthetic.pt\")\n",
        "\n",
        "# Prepare link prediction data\n",
        "synthetic_link_data = prepare_link_prediction_data(synthetic_data, seed=42)\n",
        "torch.save(synthetic_link_data, \"data/processed/synthetic_link_data.pt\")\n",
        "\n",
        "print(\"Saved synthetic dataset and link prediction data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. OGB Dataset (Optional - Large Dataset)\n",
        "\n",
        "For large-scale experiments, we can use OGB datasets. This may take longer to download and process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load OGB dataset (ogbl-collab)\n",
        "# Uncomment to download and process:\n",
        "# ogb_loader = OGBDatasetLoader(\"ogbl-collab\", root=\"data/raw/ogb\")\n",
        "# data_ogb, splits_ogb = ogb_loader.load()\n",
        "# print(f\"OGB Dataset: {data_ogb.num_nodes} nodes, {data_ogb.edge_index.size(1) // 2} edges\")\n",
        "# print(f\"Train edges: {splits_ogb['train']['edge'].shape[0]}\")\n",
        "# print(f\"Val edges: {splits_ogb['valid']['edge'].shape[0]}\")\n",
        "# print(f\"Test edges: {splits_ogb['test']['edge'].shape[0]}\")\n",
        "\n",
        "# Save OGB data\n",
        "# torch.save(data_ogb, \"data/processed/ogbl_collab.pt\")\n",
        "# torch.save(splits_ogb, \"data/processed/ogbl_collab_splits.pt\")\n",
        "# print(\"Saved OGB dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- **Facebook dataset**: Processed and saved to `data/processed/facebook_combined.pt`\n",
        "- **Link prediction data**: Saved to `data/processed/facebook_link_data.pt`\n",
        "- **Heuristics**: Computed and saved to `data/processed/facebook_heuristics.pt`\n",
        "- **Synthetic dataset**: Created for quick testing\n",
        "\n",
        "Next steps:\n",
        "1. Run baseline models (see `baselines.ipynb`)\n",
        "2. Train GNN models (see `training_graphsage_gat.ipynb` and `training_seal.ipynb`)\n",
        "3. Evaluate models (see `evaluation_and_ablation.ipynb`)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
